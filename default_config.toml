# ZDX Configuration
#
# This file is auto-generated. Edit as needed.
# Documentation: https://github.com/tallesborges/zdx#configuration

# The Claude model to use
model = "claude-haiku-4-5"

# Maximum tokens for responses (optional)
# Defaults to the model's output limit (exclusive, minus 1) when unset.
# max_tokens = 12288

# Timeout for tool execution in seconds (0 disables timeout)
tool_timeout_secs = 0

# System prompt (inline)
# system_prompt = "You are a helpful coding assistant."

# System prompt from file (takes precedence over inline)
# system_prompt_file = "/path/to/system_prompt.md"

# Extended thinking level
# Controls how much reasoning Claude shows before responding.
# Higher levels use more tokens but provide deeper reasoning.
# Options: off, minimal, low, medium, high
# Note: when max_tokens is set, it is auto-adjusted to ensure room for both thinking and response.
thinking_level = "off"

# OpenAI reasoning effort (used by Codex models).
# Options: low, medium, high, xhigh

# Provider base URLs (for proxies or test rigs).
[providers.anthropic]
enabled = true
# base_url = "https://api.anthropic.com"
models = ["claude-opus-4-5", "claude-sonnet-4-5", "claude-haiku-4-5"]

[providers.openai]
enabled = true
# base_url = "https://api.openai.com/v1"
models = ["gpt-5.2", "gpt-5-mini", "gpt-5-nano", "gpt-5.1-codex", "gpt-5.1-codex-max", "gpt-5.1-codex-mini", "gpt-4.1"]

[providers.openai_codex]
enabled = true
models = ["gpt-5.2-codex", "gpt-5.1-codex-max", "gpt-5.1-codex-mini", "gpt-5.2"]

[providers.openrouter]
enabled = true
# base_url = "https://openrouter.ai/api/v1"
models = ["*:exacto"]

[providers.gemini]
enabled = true
# base_url = "https://generativelanguage.googleapis.com/v1beta"
models = ["gemini-3-flash-preview", "gemini-3-pro-preview", "gemini-2.5-flash", "gemini-2.5-flash-lite"]

[providers.gemini_cli]
enabled = true
# base_url = "https://cloudcode-pa.googleapis.com"
models = ["gemini-3-flash-preview", "gemini-3-pro-preview", "gemini-2.5-flash", "gemini-2.5-flash-lite"]
