# Generated by zdx models update
# Edit this file to customize the model picker.

[[model]]
id = "claude-opus-4-6"
provider = "anthropic"
display_name = "Claude Opus 4.6"
context_limit = 1000000

[model.pricing]
input = 5.0
output = 25.0
cache_read = 0.5
cache_write = 6.25

[model.capabilities]
reasoning = true
input_images = true
output_limit = 128000

[[model]]
id = "claude-opus-4-5"
provider = "anthropic"
display_name = "Claude Opus 4.5"
context_limit = 200000

[model.pricing]
input = 5.0
output = 25.0
cache_read = 0.5
cache_write = 6.25

[model.capabilities]
reasoning = true
input_images = true
output_limit = 64000

[[model]]
id = "claude-sonnet-4-5"
provider = "anthropic"
display_name = "Claude Sonnet 4.5"
context_limit = 200000

[model.pricing]
input = 3.0
output = 15.0
cache_read = 0.3
cache_write = 3.75

[model.capabilities]
reasoning = true
input_images = true
output_limit = 64000

[[model]]
id = "claude-haiku-4-5"
provider = "anthropic"
display_name = "Claude Haiku 4.5"
context_limit = 200000

[model.pricing]
input = 1.0
output = 5.0
cache_read = 0.1
cache_write = 1.25

[model.capabilities]
reasoning = true
input_images = true
output_limit = 64000

[[model]]
id = "claude-cli:claude-opus-4-6"
provider = "claude-cli"
display_name = "Claude Opus 4.6"
context_limit = 1000000

[model.pricing]
input = 5.0
output = 25.0
cache_read = 0.5
cache_write = 6.25

[model.capabilities]
reasoning = true
input_images = true
output_limit = 128000

[[model]]
id = "claude-cli:claude-opus-4-5"
provider = "claude-cli"
display_name = "Claude Opus 4.5"
context_limit = 200000

[model.pricing]
input = 5.0
output = 25.0
cache_read = 0.5
cache_write = 6.25

[model.capabilities]
reasoning = true
input_images = true
output_limit = 64000

[[model]]
id = "claude-cli:claude-sonnet-4-5"
provider = "claude-cli"
display_name = "Claude Sonnet 4.5"
context_limit = 200000

[model.pricing]
input = 3.0
output = 15.0
cache_read = 0.3
cache_write = 3.75

[model.capabilities]
reasoning = true
input_images = true
output_limit = 64000

[[model]]
id = "claude-cli:claude-haiku-4-5"
provider = "claude-cli"
display_name = "Claude Haiku 4.5"
context_limit = 200000

[model.pricing]
input = 1.0
output = 5.0
cache_read = 0.1
cache_write = 1.25

[model.capabilities]
reasoning = true
input_images = true
output_limit = 64000

[[model]]
id = "openai:gpt-5.2"
provider = "openai"
display_name = "GPT-5.2"
context_limit = 400000

[model.pricing]
input = 1.75
output = 14.0
cache_read = 0.175
cache_write = 0.0

[model.capabilities]
reasoning = true
input_images = true
output_limit = 128000

[[model]]
id = "openai:gpt-5.1"
provider = "openai"
display_name = "GPT-5.1"
context_limit = 400000

[model.pricing]
input = 1.25
output = 10.0
cache_read = 0.13
cache_write = 0.0

[model.capabilities]
reasoning = true
input_images = true
output_limit = 128000

[[model]]
id = "openai:gpt-5-mini"
provider = "openai"
display_name = "GPT-5 Mini"
context_limit = 400000

[model.pricing]
input = 0.25
output = 2.0
cache_read = 0.025
cache_write = 0.0

[model.capabilities]
reasoning = true
input_images = true
output_limit = 128000

[[model]]
id = "openai:gpt-5-nano"
provider = "openai"
display_name = "GPT-5 Nano"
context_limit = 400000

[model.pricing]
input = 0.05
output = 0.4
cache_read = 0.005
cache_write = 0.0

[model.capabilities]
reasoning = true
input_images = true
output_limit = 128000

[[model]]
id = "openai:gpt-5.2-codex"
provider = "openai"
display_name = "GPT-5.2 Codex"
context_limit = 400000

[model.pricing]
input = 1.75
output = 14.0
cache_read = 0.175
cache_write = 0.0

[model.capabilities]
reasoning = true
input_images = true
output_limit = 128000

[[model]]
id = "openai:gpt-5.1-codex-max"
provider = "openai"
display_name = "GPT-5.1 Codex Max"
context_limit = 400000

[model.pricing]
input = 1.25
output = 10.0
cache_read = 0.125
cache_write = 0.0

[model.capabilities]
reasoning = true
input_images = true
output_limit = 128000

[[model]]
id = "openai:gpt-5.1-codex-mini"
provider = "openai"
display_name = "GPT-5.1 Codex mini"
context_limit = 400000

[model.pricing]
input = 0.25
output = 2.0
cache_read = 0.025
cache_write = 0.0

[model.capabilities]
reasoning = true
input_images = true
output_limit = 128000

[[model]]
id = "openai:gpt-4.1"
provider = "openai"
display_name = "GPT-4.1"
context_limit = 1047576

[model.pricing]
input = 2.0
output = 8.0
cache_read = 0.5
cache_write = 0.0

[model.capabilities]
reasoning = false
input_images = true
output_limit = 32768

[[model]]
id = "gpt-5.3-codex"
provider = "openai-codex"
display_name = "GPT-5.3 Codex"
context_limit = 400000

[model.pricing]
input = 1.75
output = 14.0
cache_read = 0.175
cache_write = 0.0

[model.capabilities]
reasoning = true
input_images = true
output_limit = 128000

[[model]]
id = "gpt-5.2-codex"
provider = "openai-codex"
display_name = "GPT-5.2 Codex"
context_limit = 400000

[model.pricing]
input = 1.75
output = 14.0
cache_read = 0.175
cache_write = 0.0

[model.capabilities]
reasoning = true
input_images = true
output_limit = 128000

[[model]]
id = "gpt-5.1-codex-max"
provider = "openai-codex"
display_name = "GPT-5.1 Codex Max"
context_limit = 400000

[model.pricing]
input = 1.25
output = 10.0
cache_read = 0.125
cache_write = 0.0

[model.capabilities]
reasoning = true
input_images = true
output_limit = 128000

[[model]]
id = "gpt-5.1-codex-mini"
provider = "openai-codex"
display_name = "GPT-5.1 Codex mini"
context_limit = 400000

[model.pricing]
input = 0.25
output = 2.0
cache_read = 0.025
cache_write = 0.0

[model.capabilities]
reasoning = true
input_images = true
output_limit = 128000

[[model]]
id = "gpt-5.2"
provider = "openai-codex"
display_name = "GPT-5.2"
context_limit = 400000

[model.pricing]
input = 1.75
output = 14.0
cache_read = 0.175
cache_write = 0.0

[model.capabilities]
reasoning = true
input_images = true
output_limit = 128000

[[model]]
id = "openrouter:deepseek/deepseek-v3.1-terminus:exacto"
provider = "openrouter"
display_name = "DeepSeek V3.1 Terminus (exacto)"
context_limit = 131072

[model.pricing]
input = 0.27
output = 1.0
cache_read = 0.0
cache_write = 0.0

[model.capabilities]
reasoning = true
input_images = false
output_limit = 65536

[[model]]
id = "openrouter:moonshotai/kimi-k2-0905:exacto"
provider = "openrouter"
display_name = "Kimi K2 Instruct 0905 (exacto)"
context_limit = 262144

[model.pricing]
input = 0.6
output = 2.5
cache_read = 0.0
cache_write = 0.0

[model.capabilities]
reasoning = false
input_images = false
output_limit = 16384

[[model]]
id = "openrouter:openai/gpt-oss-120b:exacto"
provider = "openrouter"
display_name = "GPT OSS 120B (exacto)"
context_limit = 131072

[model.pricing]
input = 0.05
output = 0.24
cache_read = 0.0
cache_write = 0.0

[model.capabilities]
reasoning = true
input_images = false
output_limit = 32768

[[model]]
id = "openrouter:qwen/qwen3-coder:exacto"
provider = "openrouter"
display_name = "Qwen3 Coder (exacto)"
context_limit = 131072

[model.pricing]
input = 0.38
output = 1.53
cache_read = 0.0
cache_write = 0.0

[model.capabilities]
reasoning = false
input_images = false
output_limit = 32768

[[model]]
id = "openrouter:z-ai/glm-4.6:exacto"
provider = "openrouter"
display_name = "GLM 4.6 (exacto)"
context_limit = 200000

[model.pricing]
input = 0.6
output = 1.9
cache_read = 0.11
cache_write = 0.0

[model.capabilities]
reasoning = true
input_images = false
output_limit = 128000

[[model]]
id = "moonshot:kimi-k2.5"
provider = "moonshot"
display_name = "Kimi K2.5"
context_limit = 262144

[model.pricing]
input = 0.6
output = 3.0
cache_read = 0.1
cache_write = 0.0

[model.capabilities]
reasoning = true
input_images = true
output_limit = 262144

[[model]]
id = "stepfun:step-3.5-flash"
provider = "stepfun"
display_name = "Step 3.5 Flash"
context_limit = 256000

[model.pricing]
input = 0.1
output = 0.3
cache_read = 0.0
cache_write = 0.0

[model.capabilities]
reasoning = true
input_images = true
output_limit = 32768

[[model]]
id = "mimo:mimo-v2-flash"
provider = "mimo"
display_name = "MiMo-V2-Flash"
context_limit = 256000

[model.pricing]
input = 0.07
output = 0.21
cache_read = 0.0
cache_write = 0.0

[model.capabilities]
reasoning = true
input_images = false
output_limit = 32000

[[model]]
id = "gemini:gemini-3-flash-preview"
provider = "gemini"
display_name = "Gemini 3 Flash Preview"
context_limit = 1048576

[model.pricing]
input = 0.5
output = 3.0
cache_read = 0.05
cache_write = 0.0

[model.capabilities]
reasoning = true
input_images = true
output_limit = 65536

[[model]]
id = "gemini:gemini-3-pro-preview"
provider = "gemini"
display_name = "Gemini 3 Pro Preview"
context_limit = 1000000

[model.pricing]
input = 2.0
output = 12.0
cache_read = 0.2
cache_write = 0.0

[model.capabilities]
reasoning = true
input_images = true
output_limit = 64000

[[model]]
id = "gemini:gemini-2.5-flash"
provider = "gemini"
display_name = "Gemini 2.5 Flash"
context_limit = 1048576

[model.pricing]
input = 0.3
output = 2.5
cache_read = 0.075
cache_write = 0.0

[model.capabilities]
reasoning = true
input_images = true
output_limit = 65536

[[model]]
id = "gemini:gemini-2.5-flash-lite"
provider = "gemini"
display_name = "Gemini 2.5 Flash Lite"
context_limit = 1048576

[model.pricing]
input = 0.1
output = 0.4
cache_read = 0.025
cache_write = 0.0

[model.capabilities]
reasoning = true
input_images = true
output_limit = 65536

[[model]]
id = "gemini-cli:gemini-3-flash-preview"
provider = "gemini-cli"
display_name = "Gemini 3 Flash Preview"
context_limit = 1048576

[model.pricing]
input = 0.5
output = 3.0
cache_read = 0.05
cache_write = 0.0

[model.capabilities]
reasoning = true
input_images = true
output_limit = 65536

[[model]]
id = "gemini-cli:gemini-3-pro-preview"
provider = "gemini-cli"
display_name = "Gemini 3 Pro Preview"
context_limit = 1000000

[model.pricing]
input = 2.0
output = 12.0
cache_read = 0.2
cache_write = 0.0

[model.capabilities]
reasoning = true
input_images = true
output_limit = 64000

[[model]]
id = "gemini-cli:gemini-2.5-flash"
provider = "gemini-cli"
display_name = "Gemini 2.5 Flash"
context_limit = 1048576

[model.pricing]
input = 0.3
output = 2.5
cache_read = 0.075
cache_write = 0.0

[model.capabilities]
reasoning = true
input_images = true
output_limit = 65536

[[model]]
id = "gemini-cli:gemini-2.5-flash-lite"
provider = "gemini-cli"
display_name = "Gemini 2.5 Flash Lite"
context_limit = 1048576

[model.pricing]
input = 0.1
output = 0.4
cache_read = 0.025
cache_write = 0.0

[model.capabilities]
reasoning = true
input_images = true
output_limit = 65536
